{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59494396-37d7-423f-a569-32b6a77b5256",
   "metadata": {},
   "source": [
    "A combined approach was chosen in order to leverage the strenght of both Hadoop and Apache Spark for an effective big data solution.\n",
    "\n",
    "The steps outlined below enabled me to initiate a connection to Apache Spark through SparkContext and launching my Spark Application. This way I am making sure that Spark is running locally and uses all available cores on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38c0085e-a8aa-4b9c-b6cc-fde973f9fb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26140315-0be6-425f-807e-49e99245a846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2cdafbe-f0e9-4ce4-b253-f17b1ff084d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, concat_ws\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660dc23-e8b6-409d-aa65-0ca9e57d4a2b",
   "metadata": {},
   "source": [
    "Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption\n",
    "\n",
    "This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70055d85-8faf-4ba1-b23a-6ce33d553954",
   "metadata": {},
   "source": [
    "After transferring the dataset to the local hadoop directory (hadoop fs -put ./household_power_consumption.txt /user1\n",
    ") and ensuring that all processes are running successfully, I can then import the data in jupyter notebook using pyspark. This approach takes full advantage of Hadoop storage capabilities and Apache Spark processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "24f45673-4dc6-484b-9e51-b70319e399fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"Global_active_power\", FloatType(), True),\n",
    "    StructField(\"Global_reactive_power\", FloatType(), True),\n",
    "    StructField(\"Voltage\", FloatType(), True),\n",
    "    StructField(\"Global_intensity\", FloatType(), True),\n",
    "    StructField(\"Sub_metering_1\", FloatType(), True),\n",
    "    StructField(\"Sub_metering_2\", FloatType(), True),\n",
    "    StructField(\"Sub_metering_3\", FloatType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/user1/household_power_consumption.txt\", header=True, schema=schema, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "287a675c-b2f6-4178-98ec-ebc7a9a880c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Global_active_power: float (nullable = true)\n",
      " |-- Global_reactive_power: float (nullable = true)\n",
      " |-- Voltage: float (nullable = true)\n",
      " |-- Global_intensity: float (nullable = true)\n",
      " |-- Sub_metering_1: float (nullable = true)\n",
      " |-- Sub_metering_2: float (nullable = true)\n",
      " |-- Sub_metering_3: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31e7be7f-09e0-4441-a687-5945445a48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "|      Date|    Time|Global_active_power|Global_reactive_power|Voltage|Global_intensity|Sub_metering_1|Sub_metering_2|Sub_metering_3|\n",
      "+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "|16/12/2006|17:24:00|              4.216|                0.418| 234.84|            18.4|           0.0|           1.0|          17.0|\n",
      "|16/12/2006|17:25:00|               5.36|                0.436| 233.63|            23.0|           0.0|           1.0|          16.0|\n",
      "|16/12/2006|17:26:00|              5.374|                0.498| 233.29|            23.0|           0.0|           2.0|          17.0|\n",
      "|16/12/2006|17:27:00|              5.388|                0.502| 233.74|            23.0|           0.0|           1.0|          17.0|\n",
      "|16/12/2006|17:28:00|              3.666|                0.528| 235.68|            15.8|           0.0|           1.0|          17.0|\n",
      "+----------+--------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5df0d3e4-2500-4cdc-a4be-db4d2dbc781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2075259\n",
      "Number of columns: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows: {df.count()}')\n",
    "print(f'Number of columns: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d4e7843-8259-423c-9371-820622cdf3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_active_power : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_reactive_power : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_intensity : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_metering_1 : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_metering_2 : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub_metering_3 : 25979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(column, \":\", df.filter(df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23916fb-f9fd-4c84-bbe3-337085800ac3",
   "metadata": {},
   "source": [
    "Missing values will be interpolated as for the time-series analysis is crucial to keep the sequence of the data. However, pyspark does not have a built-in function equivalent to the interpolate method in pandas. This is due to the distributed nature of Spark, where data is spread across multiple nodes in a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a6941-2900-45b0-b23a-798b70a96daa",
   "metadata": {},
   "source": [
    "I will combine the date and time into a single timestamp to facilitate time-series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb1b02b2-694b-4f84-b412-62da102b5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Timestamp\", to_timestamp(concat_ws(\" \", col(\"Date\"), col(\"Time\")), \"dd/MM/yyyy HH:mm:ss\")) \\\n",
    "       .drop(\"Date\", \"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e917e760-ae9a-4d47-8e74-9bfe5e872181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+-------------------+\n",
      "|Global_active_power|Global_reactive_power|Voltage|Global_intensity|Sub_metering_1|Sub_metering_2|Sub_metering_3|          Timestamp|\n",
      "+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+-------------------+\n",
      "|              4.216|                0.418| 234.84|            18.4|           0.0|           1.0|          17.0|2006-12-16 17:24:00|\n",
      "|               5.36|                0.436| 233.63|            23.0|           0.0|           1.0|          16.0|2006-12-16 17:25:00|\n",
      "|              5.374|                0.498| 233.29|            23.0|           0.0|           2.0|          17.0|2006-12-16 17:26:00|\n",
      "|              5.388|                0.502| 233.74|            23.0|           0.0|           1.0|          17.0|2006-12-16 17:27:00|\n",
      "|              3.666|                0.528| 235.68|            15.8|           0.0|           1.0|          17.0|2006-12-16 17:28:00|\n",
      "+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4ce8cb75-6287-4553-8ad1-6e17ed5bc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 316:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|Global_active_power|Global_reactive_power|           Voltage|  Global_intensity|    Sub_metering_1|    Sub_metering_2|   Sub_metering_3|\n",
      "+-------+-------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            2049280|              2049280|           2049280|           2049280|           2049280|           2049280|          2049280|\n",
      "|   mean| 1.0916150366540094|   0.1237144765251571|240.83985796672414| 4.627759313004169|1.1219233096502186|1.2985199679887571| 6.45844735712055|\n",
      "| stddev| 1.0572941611180025|  0.11272197958641315|3.2399866612063435|4.4443962589812385|  6.15303108970134| 5.822026473177461|8.437153908665614|\n",
      "|    min|              0.076|                  0.0|             223.2|               0.2|               0.0|               0.0|              0.0|\n",
      "|    25%|              0.308|                0.048|            238.99|               1.4|               0.0|               0.0|              0.0|\n",
      "|    50%|              0.602|                  0.1|            241.01|               2.6|               0.0|               0.0|              1.0|\n",
      "|    75%|              1.528|                0.194|            242.89|               6.4|               0.0|               1.0|             17.0|\n",
      "|    max|             11.122|                 1.39|            254.15|              48.4|              88.0|              80.0|             31.0|\n",
      "+-------+-------------------+---------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846623-58ec-4d66-8c01-f19ebe838c29",
   "metadata": {},
   "source": [
    "My focus will be on forecasting the global active power, meaning that all other columns are irrelevant for my analysis so they will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d8e5c577-3a63-4692-ba20-2a26400b24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"Timestamp\", \"Global_active_power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de8a72c7-483a-46c1-be78-ceb634220e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|          Timestamp|Global_active_power|\n",
      "+-------------------+-------------------+\n",
      "|2006-12-16 17:24:00|              4.216|\n",
      "|2006-12-16 17:25:00|               5.36|\n",
      "|2006-12-16 17:26:00|              5.374|\n",
      "|2006-12-16 17:27:00|              5.388|\n",
      "|2006-12-16 17:28:00|              3.666|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfc0fe-36fb-443c-87a3-4cf3e15211f3",
   "metadata": {},
   "source": [
    "The steps of the Explorary Data Analysis were conducted in order to verify the successful loading of my data into pyspark dataframe, as well as to confirm the data types and general descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bbde5d1c-bce7-446c-bf2d-26904ec2bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce062c-55cc-4267-b7f6-22e51af9f5d6",
   "metadata": {},
   "source": [
    "Importing tensorflow to perform the neural network resulted in my Kernel dying, \n",
    "\n",
    "The issue is related to the free space on my local computer. Because of that I would need to continue the modelling outside of the VM where I can import tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6acc04ed-be5b-48a9-8490-d30502385ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumption_data_path = '/user1/preprocessed_df.csv'\n",
    "\n",
    "df.write.csv(path=consumption_data_path, mode='overwrite', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b710567-f94c-4b24-bf6a-4d49de9612c5",
   "metadata": {},
   "source": [
    "Trying to import the tensorflow library in order to perform the neural network results in my Kernel dying. This is related to the free space I have on my machine. Because of that I will have to continue the analysis outside of the VM. I already exported the preprocessed dataset and it will be used in the othet jupyter notebook. There I will be able to apply the inerpolation method as well, as I will be using a pandas dataframe. As per pyspark functionalities the data was exported in two separate csv files, which later on will be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0c6e5-3cf4-4caf-a85e-773b1a9d9107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
